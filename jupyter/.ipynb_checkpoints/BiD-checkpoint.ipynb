{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random(preds, temperature=1.0):\n",
    "    #helper function to sample an index from a probability array\n",
    "    preds = preds.flatten()\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "punc = string.punctuation\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, max_length, seed_text, randomness, n_words):\n",
    "    in_text = seed_text\n",
    "    print(in_text, end=\"\")\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenNL(in_text)\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        yhat = pick_random(yhat[0], randomness)\n",
    "        \n",
    "        out_word = tokenW(yhat)\n",
    "        \n",
    "        ispunc = True\n",
    "        for c in out_word:\n",
    "            if not c in punc:\n",
    "                ispunc = False\n",
    "                break\n",
    "        \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        print(('' if ispunc else ' ') + out_word, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421553136\n",
      "39727072\n"
     ]
    }
   ],
   "source": [
    "# source text\n",
    "train = \"\"\n",
    "test = \"\"\n",
    "with open(\"../../WAFiles/blogs.txt\", 'r') as fin:\n",
    "    #for line in fin:\n",
    "    #    data += line\n",
    "    \n",
    "    for i in range(400000):\n",
    "        train += fin.readline()\n",
    "    for i in range(40000):\n",
    "        test += fin.readline()\n",
    "\n",
    "train = train.lower()\n",
    "test = test.lower()\n",
    "        \n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainl = train.split()\n",
    "trainc = collections.Counter(trainl)\n",
    "trainlist = [i for i in trainc.keys() if trainc[i] >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in trainc.keys():\n",
    "#     if trainc[i] == 2:\n",
    "#         print(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59787\n"
     ]
    }
   ],
   "source": [
    "trainsplit = train.split()\n",
    "d1 = dict(zip(range(1, len(trainlist)+1), trainlist))\n",
    "d2 = dict(zip(trainlist, range(1, len(trainlist)+1)))\n",
    "vocab_size = len(d1) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenW(n):\n",
    "    try:\n",
    "        return d1[n]\n",
    "    except:\n",
    "        return ''\n",
    "def tokenN(s):\n",
    "    try:\n",
    "        return d2[s]\n",
    "    except:\n",
    "        return 0\n",
    "def tokenWL(nums):\n",
    "    words = \"\"\n",
    "    for i in range(len(nums)):\n",
    "        words += tokenW(nums[i]) + \" \"\n",
    "    return words\n",
    "def tokenNL(words):\n",
    "    ws = words.split()\n",
    "    ar = np.empty((len(ws),))\n",
    "    for i in range(len(ws)):\n",
    "        ar[i] = tokenN(ws[i])\n",
    "    return ar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenNL(train)\n",
    "test_data = tokenNL(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81867883,)\n",
      "(7778659,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, num_steps, batch_size, total_words, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.total_words = total_words\n",
    "        self.current_idx = 0\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.total_words))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps + 10 >= len(self.data):\n",
    "                    self.current_idx = (self.current_idx + self.num_steps + 10) % len(self.data)\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x[i, self.num_steps // 2] = 0\n",
    "                temp_y = self.data[self.current_idx + self.num_steps // 2]\n",
    "                y[i, :] = tf.keras.utils.to_categorical(temp_y, num_classes=self.total_words)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = BatchGenerator(train_data, 10, batch_size, vocab_size, skip_step=1000)\n",
    "test_data_generator = BatchGenerator(test_data, 10, batch_size, vocab_size, skip_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well everyone got up and  this morning it's still - going\n"
     ]
    }
   ],
   "source": [
    "x, y = next(train_data_generator.generate())\n",
    "print(tokenWL(x[0]), end=\"- \")\n",
    "print(tokenW(np.argmax(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 128)           7652736   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2048)              9445376   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 59787)             122503563 \n",
      "=================================================================\n",
      "Total params: 139,609,867\n",
      "Trainable params: 139,605,771\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=10))\n",
    "model.add(Bidirectional(LSTM(1024)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"checkpoints/weights-{epoch:02d}-{val_loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1249/1249 [==============================] - 183s 146ms/step - loss: 4.9189 - acc: 0.2628 - val_loss: 4.7647 - val_acc: 0.2753\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 4.70603\n",
      "Epoch 2/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.9202 - acc: 0.2638 - val_loss: 4.7183 - val_acc: 0.2745\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.70603\n",
      "Epoch 3/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.9132 - acc: 0.2649 - val_loss: 4.7055 - val_acc: 0.2721\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.70603 to 4.70546, saving model to checkpoints/weights-03-4.705.hdf5\n",
      "Epoch 4/80\n",
      "1249/1249 [==============================] - 183s 146ms/step - loss: 4.8857 - acc: 0.2660 - val_loss: 4.7193 - val_acc: 0.2777\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.70546\n",
      "Epoch 5/80\n",
      "1249/1249 [==============================] - 183s 147ms/step - loss: 4.8984 - acc: 0.2668 - val_loss: 4.8467 - val_acc: 0.2701\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.70546\n",
      "Epoch 6/80\n",
      "1249/1249 [==============================] - 183s 146ms/step - loss: 4.8422 - acc: 0.2674 - val_loss: 4.7186 - val_acc: 0.2749\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.70546\n",
      "Epoch 7/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.8652 - acc: 0.2696 - val_loss: 4.7776 - val_acc: 0.2726\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.70546\n",
      "Epoch 8/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.8517 - acc: 0.2726 - val_loss: 4.6536 - val_acc: 0.2823\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.70546 to 4.65361, saving model to checkpoints/weights-08-4.654.hdf5\n",
      "Epoch 9/80\n",
      "1249/1249 [==============================] - 184s 148ms/step - loss: 4.8668 - acc: 0.2719 - val_loss: 4.7957 - val_acc: 0.2709\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.65361\n",
      "Epoch 10/80\n",
      "1249/1249 [==============================] - 185s 148ms/step - loss: 4.8657 - acc: 0.2706 - val_loss: 4.7429 - val_acc: 0.2756\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.65361\n",
      "Epoch 11/80\n",
      "1249/1249 [==============================] - 184s 147ms/step - loss: 4.8457 - acc: 0.2708 - val_loss: 4.6988 - val_acc: 0.2764\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 4.65361\n",
      "Epoch 12/80\n",
      "1249/1249 [==============================] - 183s 147ms/step - loss: 4.8295 - acc: 0.2743 - val_loss: 4.6840 - val_acc: 0.2744\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.65361\n",
      "Epoch 13/80\n",
      "1249/1249 [==============================] - 187s 150ms/step - loss: 4.8505 - acc: 0.2717 - val_loss: 4.6775 - val_acc: 0.2953\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.65361\n",
      "Epoch 14/80\n",
      "1249/1249 [==============================] - 192s 154ms/step - loss: 4.8163 - acc: 0.2774 - val_loss: 6.0029 - val_acc: 0.2239\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 4.65361\n",
      "Epoch 15/80\n",
      "1249/1249 [==============================] - 178s 142ms/step - loss: 4.7997 - acc: 0.2764 - val_loss: 4.6576 - val_acc: 0.2815\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 4.65361\n",
      "Epoch 16/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7980 - acc: 0.2775 - val_loss: 4.6376 - val_acc: 0.2887\n",
      "\n",
      "Epoch 00016: val_loss improved from 4.65361 to 4.63762, saving model to checkpoints/weights-16-4.638.hdf5\n",
      "Epoch 17/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7898 - acc: 0.2778 - val_loss: 4.6482 - val_acc: 0.2869\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 4.63762\n",
      "Epoch 18/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7854 - acc: 0.2786 - val_loss: 4.7220 - val_acc: 0.2816\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 4.63762\n",
      "Epoch 19/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7889 - acc: 0.2759 - val_loss: 4.5859 - val_acc: 0.2875\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.63762 to 4.58594, saving model to checkpoints/weights-19-4.586.hdf5\n",
      "Epoch 20/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7699 - acc: 0.2809 - val_loss: 4.6745 - val_acc: 0.2883\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.58594\n",
      "Epoch 21/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7539 - acc: 0.2790 - val_loss: 4.5869 - val_acc: 0.2974\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 4.58594\n",
      "Epoch 22/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7294 - acc: 0.2852 - val_loss: 4.6430 - val_acc: 0.2900\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.58594\n",
      "Epoch 23/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7212 - acc: 0.2844 - val_loss: 4.6108 - val_acc: 0.2826\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 4.58594\n",
      "Epoch 24/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7220 - acc: 0.2845 - val_loss: 4.6505 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.58594\n",
      "Epoch 25/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7591 - acc: 0.2816 - val_loss: 4.6799 - val_acc: 0.2889\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.58594\n",
      "Epoch 26/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7029 - acc: 0.2851 - val_loss: 4.6286 - val_acc: 0.2883\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.58594\n",
      "Epoch 27/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7368 - acc: 0.2840 - val_loss: 4.5662 - val_acc: 0.2922\n",
      "\n",
      "Epoch 00027: val_loss improved from 4.58594 to 4.56616, saving model to checkpoints/weights-27-4.566.hdf5\n",
      "Epoch 28/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7316 - acc: 0.2838 - val_loss: 4.6176 - val_acc: 0.2850\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 4.56616\n",
      "Epoch 29/80\n",
      "1249/1249 [==============================] - 180s 144ms/step - loss: 4.7348 - acc: 0.2871 - val_loss: 4.6123 - val_acc: 0.2891\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 4.56616\n",
      "Epoch 30/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.7409 - acc: 0.2858 - val_loss: 4.5917 - val_acc: 0.2905\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 4.56616\n",
      "Epoch 31/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.7528 - acc: 0.2834 - val_loss: 4.5624 - val_acc: 0.2987\n",
      "\n",
      "Epoch 00031: val_loss improved from 4.56616 to 4.56244, saving model to checkpoints/weights-31-4.562.hdf5\n",
      "Epoch 32/80\n",
      "1249/1249 [==============================] - 182s 145ms/step - loss: 4.7201 - acc: 0.2852 - val_loss: 4.5024 - val_acc: 0.3012\n",
      "\n",
      "Epoch 00032: val_loss improved from 4.56244 to 4.50237, saving model to checkpoints/weights-32-4.502.hdf5\n",
      "Epoch 33/80\n",
      "1249/1249 [==============================] - 182s 145ms/step - loss: 4.7024 - acc: 0.2897 - val_loss: 4.6297 - val_acc: 0.2907\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 4.50237\n",
      "Epoch 34/80\n",
      "1249/1249 [==============================] - 176s 141ms/step - loss: 4.7167 - acc: 0.2825 - val_loss: 4.5154 - val_acc: 0.2993\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 4.50237\n",
      "Epoch 35/80\n",
      "1249/1249 [==============================] - 177s 142ms/step - loss: 4.6869 - acc: 0.2885 - val_loss: 4.5007 - val_acc: 0.2959\n",
      "\n",
      "Epoch 00035: val_loss improved from 4.50237 to 4.50071, saving model to checkpoints/weights-35-4.501.hdf5\n",
      "Epoch 36/80\n",
      "1249/1249 [==============================] - 181s 145ms/step - loss: 4.6989 - acc: 0.2866 - val_loss: 4.6000 - val_acc: 0.2954\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 4.50071\n",
      "Epoch 37/80\n",
      "1249/1249 [==============================] - 179s 144ms/step - loss: 4.6961 - acc: 0.2897 - val_loss: 4.5633 - val_acc: 0.2966\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 4.50071\n",
      "Epoch 38/80\n",
      "1249/1249 [==============================] - 182s 146ms/step - loss: 4.7222 - acc: 0.2896 - val_loss: 4.5814 - val_acc: 0.2956\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 4.50071\n",
      "Epoch 39/80\n",
      "1249/1249 [==============================] - 186s 149ms/step - loss: 4.6828 - acc: 0.2927 - val_loss: 4.5905 - val_acc: 0.2940\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 4.50071\n",
      "Epoch 40/80\n",
      "1249/1249 [==============================] - 184s 147ms/step - loss: 4.7005 - acc: 0.2907 - val_loss: 4.4524 - val_acc: 0.3108\n",
      "\n",
      "Epoch 00040: val_loss improved from 4.50071 to 4.45238, saving model to checkpoints/weights-40-4.452.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-40feae2a39ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, attr, val)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlgpu/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit_generator(\n",
    "    generator=train_data_generator.generate(),\n",
    "    steps_per_epoch=len(train_data)//(batch_size)//1024,\n",
    "    epochs=80,\n",
    "    validation_data=test_data_generator.generate(),\n",
    "    validation_steps = len(test_data)//(batch_size)//1024,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_tmp.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full of craziness now unable ~to~ focus he seemed on \n",
      "the pressure my attorney has ~not~ been able to accept \n",
      "butte jacket booming through the ~~ island tunnel at the \n",
      "thing to get these days ~so~ i figured well just \n",
      "a savage invitation from the ~~ suddenly i felt guilty \n",
      "south towards l a but ~at~ all deliberate speed keep \n",
      "morning rush of pimps and ~the~ hustlers with a huge \n",
      "the devil keep that in ~and~ buy the ticket take \n",
      "advantage of that rest area ~should~ i tell you how \n",
      "the chp and then with ~the~ filthy phantom hitchhiker plunging \n",
      "straight out to his car ~and~ start abusing those drugs \n",
      "esoteric lights dials meters that ~i~ would never understand but \n",
      "to the desk clerk i ~really~ hate to interrupt but \n",
      "the famous journalist pairing for ~the~ suite lucy on our \n",
      "herself into a towering jesus ~of~ rage at the hazy \n",
      "to the airport saying we ~are~ going to trade the \n",
      "to that woman she was ~in~  i think she \n",
      "other one met us at ~the~ hotel he was sweating \n",
      "he was back in his ~car~ watching mission impossible and \n",
      "not free of the drug ~the~ voltage had merely been \n"
     ]
    }
   ],
   "source": [
    "x = np.array(next(train_data_generator.generate())[0])\n",
    "y = model.predict(x)\n",
    "\n",
    "printNum = 20\n",
    "\n",
    "for b in range(printNum):\n",
    "    gen = \"\"\n",
    "    for i in range(len(x[b])):\n",
    "        w = tokenW(x[b,i])\n",
    "        # print('-' if w=='' else w, end=\" \")\n",
    "        gen += ('%*' if i==len(x[b])//2 else w) + \" \"\n",
    "    # print()\n",
    "    # print(tokenW(np.argmax(y)))\n",
    "    print(gen.replace(\"%*\", (\"~\"+tokenW(np.argmax(y[b]))+\"~\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([d1, d2], open('tokenizer_tmp.dat', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
