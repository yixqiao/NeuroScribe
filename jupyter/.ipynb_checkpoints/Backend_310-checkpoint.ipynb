{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tokenizer = pickle.load(open('tokenizer_311.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 64)            3475136   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              4460544   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54299)             55656475  \n",
      "=================================================================\n",
      "Total params: 63,596,251\n",
      "Trainable params: 63,594,203\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = load_model('model_311.hdf5')\n",
    "model.summary()\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'This', 'eval': 0.0033495547, 'best': 0.03351025, 'bestw': 'and'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'post', 'eval': 0.005488346, 'best': 0.36798516, 'bestw': 'is'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'details', 'eval': 2.6623029e-05, 'best': 0.08070713, 'bestw': 'is'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'my', 'eval': 0.0018156278, 'best': 0.047657646, 'bestw': 'and'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'experience',\n",
       "  'eval': 0.0001419951,\n",
       "  'best': 0.07237378,\n",
       "  'bestw': 'head'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'compiling',\n",
       "  'eval': 1.006998e-07,\n",
       "  'best': 0.11112437,\n",
       "  'bestw': 'is'},\n",
       " {'word': ' Aseprite ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'for', 'eval': 0.022323413, 'best': 0.04373668, 'bestw': 'is'},\n",
       " {'word': ' Ubuntu. ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'It', 'eval': 0.010467096, 'best': 0.065639235, 'bestw': 'a'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'is', 'eval': 0.043166813, 'best': 0.055114716, 'bestw': 'i'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'nice', 'eval': 0.001640055, 'best': 0.06484478, 'bestw': 'all'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a sequence from a language model\n",
    "def processpost(text):\n",
    "    max_length = 10\n",
    "    \n",
    "    encoded_full = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    encoded_text = []\n",
    "    for i in range(len(encoded_full)):\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == encoded_full[i]:\n",
    "                encoded_text.append(word)\n",
    "                break\n",
    "    text_lower = text.lower()\n",
    "    encoded = []\n",
    "    out = []\n",
    "    \n",
    "    prevLoc = 0\n",
    "    for i in range(len(encoded_full)):\n",
    "        encoded = pad_sequences([encoded_full[:i]], maxlen=max_length, padding='pre')\n",
    "        # print(encoded)\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        curword = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == encoded_full[i]:\n",
    "                curword = word\n",
    "                break\n",
    "        \n",
    "        maxw = np.argmax(yhat)\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == maxw:\n",
    "                maxw = word\n",
    "                break\n",
    "        \n",
    "        maxw = np.argmax(yhat)\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == maxw:\n",
    "                maxw = word\n",
    "                break\n",
    "        \n",
    "        #print(\"%8.4f (%6.3f / %6.3f) %s\" % (yhat[0,encoded_full[i]]/np.amax(yhat) * 100,\n",
    "        #                                      yhat[0,encoded_full[i]] * 100, np.amax(yhat) * 100, maxw))\n",
    "        \n",
    "\n",
    "        # print(\"%6.3f / %6.3f\" % (yhat[0,encoded_full[i]] * 100, np.amax(yhat) * 100))\n",
    "        prevEnd = text_lower[prevLoc:].find(curword) + prevLoc\n",
    "        if(prevLoc < prevEnd):\n",
    "            out.append({\"word\": text[prevLoc:prevEnd], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "        out.append({\"word\": text[prevEnd:prevEnd+len(curword)], \"eval\": yhat[0,encoded_full[i]], \"best\": np.amax(yhat), \"bestw\": maxw})\n",
    "        prevLoc = prevEnd + len(curword)\n",
    "    out.append({\"word\": text[prevLoc:], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "    return out\n",
    "    \n",
    "\n",
    "    \n",
    "processpost(\"\"\"This post details my experience compiling Aseprite for Ubuntu. It is nice\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
