{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import string\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "d1, d2 = pickle.load(open('tokenizer_410.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenW(n):\n",
    "    try:\n",
    "        return d1[n]\n",
    "    except:\n",
    "        return ''\n",
    "def tokenN(s):\n",
    "    try:\n",
    "        return d2[s]\n",
    "    except:\n",
    "        return 0\n",
    "def tokenWL(nums):\n",
    "    words = \"\"\n",
    "    for i in range(len(nums)):\n",
    "        words += tokenW(nums[i]) + \" \"\n",
    "    return words\n",
    "def tokenNL(words):\n",
    "    ws = words.split()\n",
    "    ar = np.empty((len(ws),))\n",
    "    for i in range(len(ws)):\n",
    "        ar[i] = tokenN(ws[i])\n",
    "    return ar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yixuan/miniconda3/envs/mlgpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 128)           7652736   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2048)              9445376   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 59787)             122503563 \n",
      "=================================================================\n",
      "Total params: 139,609,867\n",
      "Trainable params: 139,605,771\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = load_model('model_410.hdf5')\n",
    "model.summary()\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = \"\"\"a about above after again against all am an and any are aren't as at be because been before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't you you'd you'll you're you've your yours yourself yourselves\"\"\".split()\n",
    "stopwords += \"urllink\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(post):\n",
    "    goodChars = string.ascii_letters + string.digits + string.punctuation\n",
    "    punc = \"\"\" .,'\"!() \"\"\"\n",
    "    stPunc = string.punctuation\n",
    "    \n",
    "    post = post.strip()\n",
    "    post = post.translate(str.maketrans(\"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\", \" \" * len(\"\"\"!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"))) # No apostrophes                \n",
    "\n",
    "    ps = post.split()\n",
    "    ps = post.split()\n",
    "    ps2 = ps # Split punctuation\n",
    "\n",
    "    ps3 = [] # Remove only punctuation\n",
    "    for p in ps2:\n",
    "        allPunc = True\n",
    "        for c in p:\n",
    "            if not c in stPunc:\n",
    "                allPunc = False\n",
    "                break\n",
    "\n",
    "        if allPunc:\n",
    "            continue\n",
    "        ps3 += p.split()\n",
    "\n",
    "    return \" \".join(ps3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'I', 'eval': 0.54237413, 'best': 0.014316103, 'bestw': 'will'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'feel', 'eval': 0.0066566803, 'best': 0.0253514, 'bestw': 'think'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'amazing',\n",
       "  'eval': 0.0004988556,\n",
       "  'best': 0.32696944,\n",
       "  'bestw': 'like'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'today',\n",
       "  'eval': 0.0075757448,\n",
       "  'best': 0.07856524,\n",
       "  'bestw': 'sometimes'},\n",
       " {'word': '. ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'I', 'eval': 0.9817755, 'best': 0.001291229, 'bestw': 'dont'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'think',\n",
       "  'eval': 0.087539524,\n",
       "  'best': 0.21821348,\n",
       "  'bestw': 'decided'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'that', 'eval': 0.39335713, 'best': 0.23912641, 'bestw': 'maybe'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'I', 'eval': 0.7980985, 'best': 0.004493992, 'bestw': 'people'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'could', 'eval': 0.03493931, 'best': 0.24358355, 'bestw': 'will'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'go', 'eval': 0.23018962, 'best': 0.23018962, 'bestw': 'go'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'out', 'eval': 0.1753941, 'best': 0.03248135, 'bestw': 'back'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'there', 'eval': 0.3526704, 'best': 0.061027892, 'bestw': 'now'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'and', 'eval': 0.6762584, 'best': 0.0034041882, 'bestw': 'just'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'do', 'eval': 0.63630456, 'best': 0.05472391, 'bestw': 'say'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'whatever',\n",
       "  'eval': 0.035388444,\n",
       "  'best': 0.18713497,\n",
       "  'bestw': 'something'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'I', 'eval': 0.02377683, 'best': 0.039053652, 'bestw': 'everyone'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'wants', 'eval': 0.0015395824, 'best': 0.3482878, 'bestw': 'want'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'to', 'eval': 0.17182799, 'best': 0.0072666258, 'bestw': 'like'},\n",
       " {'word': '.', 'eval': -1, 'best': -1, 'bestw': ''}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processpost(text):\n",
    "    max_length = 10\n",
    "    ml2 = max_length//2\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    encoded_full = tokenNL(cleanup(text_lower))\n",
    "    \n",
    "    encoded_text = tokenWL(encoded_full)\n",
    "    \n",
    "    encoded = []\n",
    "    out = []\n",
    "    \n",
    "    prevLoc = 0\n",
    "\n",
    "    inputdata = np.zeros((len(encoded_full),10))\n",
    "    for i in range(len(encoded_full)):\n",
    "        inputdata[i, max(ml2-i,0):min(len(encoded_full)-i+ml2,max_length)] = encoded_full[max(i-ml2,0):min(i+ml2,len(encoded_full))]\n",
    "        inputdata[i, 5] = 0\n",
    "\n",
    "    # predict probabilities for each word\n",
    "        #with graph.as_default():\n",
    "        #    set_session(sess)\n",
    "    yhat_all = model.predict(inputdata, batch_size=32, verbose=0)\n",
    "        \n",
    "    for i in range(len(encoded_full)):\n",
    "        yhat_all\n",
    "        yhat = yhat_all[i]\n",
    "                    \n",
    "        curword = tokenW(encoded_full[i])\n",
    "        \n",
    "        evalv = yhat[int(encoded_full[i])]\n",
    "        bestv = np.amax(yhat)\n",
    "\n",
    "        maxw = np.argmax(yhat)\n",
    "        while(maxw==0 or tokenW(maxw).strip() in stopwords):\n",
    "            yhat[maxw] = -1000\n",
    "            maxw = np.argmax(yhat)\n",
    "\n",
    "        maxw = tokenW(maxw)\n",
    "        \n",
    "        #print(\"%-8s - %8.4f (%6.3f / %6.3f) %s\" % (curword, yhat[0,int(encoded_full[i])]/np.amax(yhat) * 100,\n",
    "        #                                      yhat[0,int(encoded_full[i])] * 100, np.amax(yhat) * 100, maxw))\n",
    "        \n",
    "\n",
    "        # print(\"%6.3f / %6.3f\" % (yhat[0,int(encoded_full[i])] * 100, np.amax(yhat) * 100))\n",
    "        \n",
    "        prevEnd = text_lower[prevLoc:].find(curword) + prevLoc\n",
    "        if(prevLoc < prevEnd):\n",
    "            out.append({\"word\": text[prevLoc:prevEnd], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "        out.append({\"word\": text[prevEnd:prevEnd+len(curword)], \"eval\": evalv, \"best\": np.amax(yhat), \"bestw\": maxw})\n",
    "        prevLoc = prevEnd + len(curword)\n",
    "    out.append({\"word\": text[prevLoc:], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "    return out\n",
    "\n",
    "processpost(\"\"\"I feel amazing today. I think that I could go out there and do whatever I wants to.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': \"i'm\", 'eval': 0.011101731, 'best': 0.08082738, 'bestw': ''},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'trying', 'eval': 0.10991073, 'best': 0.15389693, 'bestw': 'going'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'to', 'eval': 0.54134446, 'best': 0.54134446, 'bestw': 'to'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'think', 'eval': 0.005912985, 'best': 0.17823455, 'bestw': 'see'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'if', 'eval': 0.57610375, 'best': 0.57610375, 'bestw': 'if'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': \"you've\", 'eval': 0.4656869, 'best': 0.4656869, 'bestw': \"you've\"},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'ever', 'eval': 0.6734039, 'best': 0.6734039, 'bestw': 'ever'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'heard', 'eval': 0.047287162, 'best': 0.23496832, 'bestw': 'had'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'this', 'eval': 0.0145301325, 'best': 0.06681914, 'bestw': 'of'},\n",
       " {'word': ' ', 'eval': -1, 'best': -1, 'bestw': ''},\n",
       " {'word': 'story',\n",
       "  'eval': 0.00091182353,\n",
       "  'best': 0.043449342,\n",
       "  'bestw': 'urllink'},\n",
       " {'word': '', 'eval': -1, 'best': -1, 'bestw': ''}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a sequence from a language model\n",
    "def processpost(text):\n",
    "    max_length = 10\n",
    "    ml2 = max_length//2\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    encoded_full = tokenNL(cleanup(text_lower))\n",
    "    \n",
    "    encoded_text = tokenWL(encoded_full)\n",
    "    \n",
    "    encoded = []\n",
    "    out = []\n",
    "    \n",
    "    prevLoc = 0\n",
    "    for i in range(len(encoded_full)):\n",
    "        encoded = np.zeros((10))\n",
    "        encoded[max(ml2-i,0):min(len(encoded_full)-i+ml2,max_length)] = encoded_full[max(i-ml2,0):min(i+ml2,len(encoded_full))]\n",
    "\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict(encoded.reshape(1,max_length), verbose=0)\n",
    "                \n",
    "        curword = tokenW(encoded_full[i])\n",
    "        \n",
    "        maxw = tokenW(np.argmax(yhat))\n",
    "        \n",
    "        #print(\"%-8s - %8.4f (%6.3f / %6.3f) %s\" % (curword, yhat[0,int(encoded_full[i])]/np.amax(yhat) * 100,\n",
    "        #                                      yhat[0,int(encoded_full[i])] * 100, np.amax(yhat) * 100, maxw))\n",
    "        \n",
    "\n",
    "        # print(\"%6.3f / %6.3f\" % (yhat[0,int(encoded_full[i])] * 100, np.amax(yhat) * 100))\n",
    "        \n",
    "        prevEnd = text_lower[prevLoc:].find(curword) + prevLoc\n",
    "        if(prevLoc < prevEnd):\n",
    "            out.append({\"word\": text[prevLoc:prevEnd], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "        out.append({\"word\": text[prevEnd:prevEnd+len(curword)], \"eval\": yhat[0,int(encoded_full[i])], \"best\": np.amax(yhat), \"bestw\": maxw})\n",
    "        prevLoc = prevEnd + len(curword)\n",
    "    out.append({\"word\": text[prevLoc:], \"eval\": -1, \"best\": -1, \"bestw\": \"\"})\n",
    "    return out\n",
    "    \n",
    "\n",
    "    \n",
    "processpost(\"\"\"i'm trying to think if you've ever heard this story\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
